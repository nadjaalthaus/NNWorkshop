{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d41524-19fb-481a-a37e-e0f9dc9ba3be",
   "metadata": {},
   "source": [
    "# Connectionist modelling workshop \n",
    "## Learning the English Past Tense\n",
    "\n",
    "In this worksheet we'll build a neural network to simulate Past Tense acquisition in English. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd55394-a764-44c4-b2ba-83b417603791",
   "metadata": {},
   "source": [
    "### Prepare the code\n",
    "\n",
    "Run this cell at the beginning to load necessary modules and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa9754-53e6-4de8-a0be-bc0ea3d48a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL1\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import seed, random\n",
    "import random\n",
    "from csv import reader\n",
    "\n",
    "# Load a CSV file with numeric content\n",
    "def load_csv_numeric(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "\n",
    "            dataset.append([float(x) for x in row])\n",
    "    return dataset\n",
    "\n",
    "#load a csv file with char or string content\n",
    "def load_csv_str(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "\n",
    "            if not row:\n",
    "                continue\n",
    "\n",
    "            dataset.append(row[0])\n",
    "    return dataset\n",
    "\n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + np.exp(-activation))\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = []\n",
    "    hidden_layer = [{'weights':[random.random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random.random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    print('architecture:')\n",
    "    print('input units:'+str(n_inputs))\n",
    "    print('hidden units:'+str(n_hidden))\n",
    "    print('output units:'+str(n_outputs))\n",
    "    print('------------------')\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "def network(train, traintarget, test, testtarget, l_rate, n_epoch, n_hidden, momentum=0.9,plot_error=False):\n",
    "    print('initialize network')\n",
    "    n_inputs = len(train[0])\n",
    "    n_outputs = len(traintarget[0])\n",
    "\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    \n",
    "    trainerror=train_backprop(network, train,traintarget, l_rate, n_epoch, momentum, plot_error)\n",
    "    testerror=test_network(network, test, testtarget)\n",
    "    return network, trainerror, testerror\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_backprop(network, train, traintarget, l_rate, n_epoch, momentum=.9, plot_error=False, cat_vector=[]):\n",
    "    print('train_backprop')\n",
    "    rms_error=[]\n",
    "    if len(cat_vector)>0:\n",
    "        cats=list(set(cat_vector))\n",
    "        print(cats)\n",
    "        ncats=len(cats)\n",
    "        cat_errors=[]\n",
    "        for c in range(ncats):\n",
    "            cat_errors.append([])\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        print('epoch:'+str(epoch))\n",
    "        sum_error = 0\n",
    "        if len(cat_vector)>0:\n",
    "            cat_sum_errors= [0] * ncats\n",
    "            cat_sum_n=[0]*ncats\n",
    "        for j in range(len(train)):\n",
    "\n",
    "            if len(cat_vector)>0:\n",
    "                this_cat=cat_vector[j]\n",
    "                this_cat_idx=cats.index(this_cat)\n",
    "            outputs = forward_propagate(network, train[j])\n",
    "            #print('got outputs')\n",
    "            expected = traintarget[j]\n",
    "\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            if len(cat_vector)>0:\n",
    "                cat_sum_errors[this_cat_idx] += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "                cat_sum_n[this_cat_idx] +=1\n",
    "            backward_propagate_error(network, expected)\n",
    "            if momentum>0:\n",
    "                update_weights_momentum(network, train[j], l_rate, momentum)\n",
    "            else:\n",
    "\t            update_weights(network, train[j], l_rate)\n",
    "        epoch_rms_error = np.sqrt(sum_error/len(train))\n",
    "        rms_error.append(epoch_rms_error)\n",
    "        if len(cat_vector)>0:\n",
    "\n",
    "            for c in range(ncats):\n",
    "                epoch_cat_rms_error=np.sqrt(cat_sum_errors[c]/cat_sum_n[c])\n",
    "                cat_errors[c].append(epoch_cat_rms_error)\n",
    "\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, epoch_rms_error))\n",
    "    if plot_error:\n",
    "        if len(cat_vector)>0:\n",
    "            plt.figure()\n",
    "            plt.plot(range(n_epoch), rms_error)\n",
    "            for c in range(ncats):\n",
    "                plt.plot(range(n_epoch),cat_errors[c])\n",
    "            plt.legend(['overall']+cats)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('error')\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.figure()\n",
    "            plt.plot(range(n_epoch), rms_error)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('error')\n",
    "            plt.show()\n",
    "    return rms_error[-1]\n",
    "\n",
    "\n",
    "# Neural network using Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "#training phase only\n",
    "#option for showing error by category\n",
    "def network_train_only(train, traintarget, l_rate, n_epoch, n_hidden, momentum=0.9,plot_error=False, cat_vector=[]):\n",
    "    print('initialize network')\n",
    "    n_inputs = len(train[0])\n",
    "    n_outputs = len(traintarget[0])\n",
    "\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    \n",
    "    trainerror=train_backprop(network, train,traintarget, l_rate, n_epoch, momentum, plot_error,cat_vector)\n",
    "    return network, trainerror\n",
    "\n",
    "#calculate error for test set (frozen weights, no training)\n",
    "def test_network(network, test, testtarget):\n",
    "    testoutputs=[]\n",
    "    error_sum = 0\n",
    "    for i in range(len(test)):\n",
    "        this_testoutput = predict(network, test[i])\n",
    "        testoutputs.append(this_testoutput)\n",
    "\n",
    "        this_target=testtarget[i]\n",
    "        error_sum+=sum([(this_target[i]-this_testoutput[i])**2 for i in range(len(this_target))])\n",
    "    rms_error=np.sqrt(error_sum/len(test))\n",
    "    return rms_error\n",
    "\n",
    "# Update network weights with momentum\n",
    "def update_weights_momentum(network, row, l_rate, momentum):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                if not 'delta_prev' in neuron.keys():\n",
    "                    neuron['weights'][j] += (l_rate * neuron['delta'] * inputs[j])\n",
    "                    neuron['delta_prev']=[0 for i in range(len(inputs))]\n",
    "                    neuron['delta_prev'][j] = (l_rate * neuron['delta'] * inputs[j])\n",
    "                else:\n",
    "                    neuron['weights'][j] += (l_rate * neuron['delta'] * inputs[j]) + (momentum*neuron['delta_prev'][j])\n",
    "                    neuron['delta_prev'][j]=(l_rate * neuron['delta'] * inputs[j]) + (momentum*neuron['delta_prev'][j])\n",
    "            neuron['weights'][-1] += (l_rate * neuron['delta']) + (momentum*neuron['delta_prev'][-1])\n",
    "            neuron['delta_prev'][-1]=(l_rate * neuron['delta']) + (momentum*neuron['delta_prev'][-1])\n",
    "\n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "            \n",
    "            \n",
    "def backward_propagate_error(network, expected):\n",
    "\n",
    "    for i in reversed(range(len(network))):\n",
    "\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "            \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    #print('forwardprop')\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs\n",
    "\n",
    "#show the output for a single test pattern (given by idx)\n",
    "def test_single_pattern(network, test, testtarget, testlabels, testtargetlabels, idx):\n",
    "    this_testoutput=predict(network, test[idx])\n",
    "    this_target = testtarget[idx]\n",
    "    error_pattern = [(this_target[i] - this_testoutput[i]) for i in range(len(this_target))]\n",
    "\n",
    "\n",
    "    x = np.arange(len(this_target))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width / 2, this_target, width, label='Target')\n",
    "    rects2 = ax.bar(x + width / 2, this_testoutput, width, label='Output')\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Output activation')\n",
    "    ax.set_title('Target vs. Output for pattern ' +str(idx) +':' + testlabels[idx] + '/' + testtargetlabels[idx])\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    return this_testoutput, error_pattern\n",
    "\n",
    "#train a network and plot the error for a single pattern given by idx\n",
    "def network_log_pattern(train, traintarget,  trainlabel, traintargetlabel,l_rate, n_epoch, n_hidden, idx=0, momentum=0.9):\n",
    "    print('call network')\n",
    "    n_inputs = len(train[0])\n",
    "    n_outputs = len(traintarget[0])\n",
    "\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "\n",
    "    trainerror = train_backprop_log_pattern(network, train, traintarget, trainlabel, traintargetlabel, l_rate, n_epoch, idx, momentum)\n",
    "\n",
    "    return network, trainerror\n",
    "\n",
    "\n",
    "#run backprop and plot the error for a single pattern given by idx\n",
    "def train_backprop_log_pattern(network, train, traintarget, trainlabel, traintargetlabel, l_rate, n_epoch, idx=0, momentum=0.9, plot_error=False):\n",
    "    print('train_backprop')\n",
    "    rms_error=[]\n",
    "    pattern_error=[]\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        print('epoch:'+str(epoch))\n",
    "        sum_error = 0\n",
    "\n",
    "        for j in range(len(train)):\n",
    "\n",
    "\n",
    "\n",
    "            outputs = forward_propagate(network, train[j])\n",
    "            #print('got outputs')\n",
    "            expected = traintarget[j]\n",
    "            if j==idx:\n",
    "                pattern_error.append(sum([(expected[i]-outputs[i])**2 for i in range(len(expected))]))\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "\n",
    "            backward_propagate_error(network, expected)\n",
    "            if momentum>0:\n",
    "                update_weights_momentum(network, train[j], l_rate, momentum)\n",
    "            else:\n",
    "\t            update_weights(network, train[j], l_rate)\n",
    "        epoch_rms_error = np.sqrt(sum_error/len(train))\n",
    "        rms_error.append(epoch_rms_error)\n",
    "\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.6f' % (epoch, l_rate, epoch_rms_error))\n",
    "\n",
    "        #print(len(pattern_error))\n",
    "        #print(len(range(n_epoch)))\n",
    "    plt.figure()\n",
    "    plt.plot(range(n_epoch), pattern_error)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('error')\n",
    "    plt.title('error for pattern: '+trainlabel[idx] + '/' + traintargetlabel[idx])\n",
    "    plt.show()\n",
    "    return rms_error[-1]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f982c-e5a5-46d3-a102-36180ca1ea90",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "One of the main points of the past tense simulation is generalisability, or the realisation that as human speakers of English we can easily create a past tense form for a new word, like wug --> wugged. As discussed in the lecture we're going to use a training set and a separate test set, so that we can train the model on some data but test with unseen cases how well the model generalises.\n",
    "\n",
    "The following code will create eight variables:<br>\n",
    "`trainInput` (numeric input to neural network for training)<br>\n",
    "`trainInputLabels` (transcriptions of input to neural network / training)<br>\n",
    "`trainTarget` (numeric target output for neural network/ training)<br>\n",
    "`trainTargetLabels` (transcriptions of target output / training)<br>\n",
    "`testInput` (numeric input to neural network for testing)<br>\n",
    "`testInputLabels` (transcriptions of input for testing)<br>\n",
    "`testTarget` (numeric target outputs for testing)<br>\n",
    "`testTargetLabels` (transcriptions of target for testing)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdfa7f-da91-4040-a1ad-34bad66b6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL2\n",
    "\n",
    "\n",
    "trainInput = load_csv_numeric('PastTenseData/input.csv')\n",
    "trainInputLabels= load_csv_str('PastTenseData/inputlabels.csv')\n",
    "trainTarget = load_csv_numeric('PastTenseData/target.csv')\n",
    "trainTargetLabels= load_csv_str('PastTenseData/targetlabels.csv')\n",
    "testInput = load_csv_numeric('PastTenseData/testInput.csv')\n",
    "testInputLabels= load_csv_str('PastTenseData/testInputLabels.csv')\n",
    "testTarget = load_csv_numeric('PastTenseData/testTarget.csv')\n",
    "testTargetLabels= load_csv_str('PastTenseData/testTargetLabels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119068f0-793f-4801-abfb-dff2931ca683",
   "metadata": {},
   "source": [
    "### Inspect the data\n",
    "\n",
    "Run the following cell to display, for all the loaded variables, their size. Make sure you understand why they have this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf1809-46a0-4450-a370-a8c7d01aa0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 3\n",
    "\n",
    "print('trainInput size:'+str(len(trainInput)))\n",
    "print('pattern length:'+str(len(trainInput[0])))\n",
    "print('--')\n",
    "print('trainTarget size:'+str(len(trainTarget)))\n",
    "print('pattern length:'+str(len(trainTarget[0])))\n",
    "print('--')\n",
    "print('testInput size:'+str(len(testInput)))\n",
    "print('pattern length:'+str(len(testInput[0])))\n",
    "print('--')\n",
    "print('testTarget size:'+str(len(testTarget)))\n",
    "print('pattern length:'+str(len(testTarget[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82faf75-80c2-4c68-9af6-99184c0b60b1",
   "metadata": {},
   "source": [
    "As you probably guessed, we have a training set of 450 input patterns with corresponding targets, and a test set of 50 input patterns with corresponding targets. Those are not going to be included in training, so that we can test generalisation.\n",
    "\n",
    "The training patterns come in four categories, for convenience referred to as W,X,Y,Z. <br>\n",
    "W: vowelchange verbs (e.g. sing/sang)<br>\n",
    "X: no-change verbs (e.g. hit/hit)<br>\n",
    "Y: regular verbs with voiced suffix (e.g. smile/smiled)<br>\n",
    "Z: regular verbs with voiceless suffix (e.g. kiss/kissed)<br>\n",
    "The following cell displays two patterns and their targets per category. Make sure you understand how the stem/suffix are represented. \n",
    "**Tip:** You may have to resize the window to display the patterns neatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05929797-8ee4-4f13-ac99-3ddd2b75e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL4 \n",
    "\n",
    "print('W: vowelchange')\n",
    "print(trainInput[7])\n",
    "print(trainTarget[7])\n",
    "print(trainInputLabels[7] + '-'+trainTargetLabels[7])\n",
    "print('--')\n",
    "print(trainInput[39])\n",
    "print(trainTarget[39])\n",
    "print(trainInputLabels[39] + '-'+trainTargetLabels[39])\n",
    "print('%%%%%%%%%%%%')\n",
    "print('X: no change')\n",
    "print(trainInput[10])\n",
    "print(trainTarget[10])\n",
    "print(trainInputLabels[10] + '-'+trainTargetLabels[10])\n",
    "print('--')\n",
    "print(trainInput[211])\n",
    "print(trainTarget[211])\n",
    "print(trainInputLabels[211] + '-'+trainTargetLabels[211])\n",
    "print('%%%%%%%%%%%%')\n",
    "print('Y: voiced -ed')\n",
    "print(trainInput[2])\n",
    "print(trainTarget[2])\n",
    "print(trainInputLabels[2] + '-'+trainTargetLabels[2])\n",
    "print('--')\n",
    "print(trainInput[4])\n",
    "print(trainTarget[4])\n",
    "print(trainInputLabels[4] + '-'+trainTargetLabels[4])\n",
    "print('%%%%%%%%%%%%')\n",
    "print('Y: voiceless -ed')\n",
    "print(trainInput[0])\n",
    "print(trainTarget[0])\n",
    "print(trainInputLabels[0] + '-'+trainTargetLabels[0])\n",
    "print('--')\n",
    "print(trainInput[5])\n",
    "print(trainTarget[5])\n",
    "print(trainInputLabels[5] + '-'+trainTargetLabels[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a56d2-4ac2-4629-887b-a8102858898e",
   "metadata": {},
   "source": [
    "We're going to set up a network with 20 hidden units that we'll use to train to associate the stems from the training set with their corresponding past tense form. <br>\n",
    "**Task 1:** How many input and output units are we going to need? Try to answer before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47548b3e-be54-490d-96a5-e1d237053e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 5\n",
    "\n",
    "[this_network, trainerror, testerror]=network(trainInput, trainTarget, testInput, testTarget, l_rate=.1, n_epoch=100,n_hidden=20, plot_error=True)\n",
    "print('final training error:'+str(round(trainerror,2)))\n",
    "print('final test error:'+str(round(testerror,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40705e85-d404-459f-ab4d-30f1deef0af6",
   "metadata": {},
   "source": [
    "**Task 2:** Did this network train successfully? How do you know?<br>\n",
    "**Task 3:** In CELL5, change the learning rate, number of epochs and number of hidden units repeatedly. Try a few different combinations. Don't forget to write down the final training/testing error. Can you improve the performance beyond your initial run? (You may want to continue your search for optimal parameters at home so that you can get through more tasks in class. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30021f75-b7b2-4063-adb1-8ca0b9a9ad05",
   "metadata": {},
   "source": [
    "***\n",
    "Run the network in CELL5 with the following parameters:<br>\n",
    "`l_rate`=0.1<br>\n",
    "`n_epoch`=100<br>\n",
    "`n_hidden`=20<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730abd2a-5a45-4875-9805-6814ee06103e",
   "metadata": {},
   "source": [
    "**Task 4:** Describe the training progress. In terms of training error, what would be the optimal time to stop training? Run the network again, with that number of epochs. Was this also good for the test error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996b847-4730-4189-86e7-7ec8da07d67b",
   "metadata": {},
   "source": [
    "So far, we have only talked about quality in terms of error, but does the network actually get the answers right? Let's inspect individual patterns.\n",
    "**Run CELL 6 to see the target and actual output for the first item in the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531748b-8169-4830-bbc3-d157806282fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 6\n",
    "idx = 6\n",
    "[output_pattern, error_pattern]=test_single_pattern(this_network, testInput, testTarget,testInputLabels, testTargetLabels, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ddfd3-0188-4b86-9a73-d294207e4a2e",
   "metadata": {},
   "source": [
    "Now, repeat CELL6 but change the index `idx` (you may have to scroll to the right to see how it is used) to see the performance of the network for other patterns. Try at least one pattern from each category W,X,Y,Z (hint: you can find patterns on the basis of their categories by looking at the file TestTargetLabels.csv in excel, but remember that python starts counting at 0, so for the pattern displayed in row 5 in excel, change idx in CELL6 to 4.)\n",
    "\n",
    "**Task 5:** How is the network doing particularly on the final two test outputs which correspond to the category? How is it performing for vowel change patterns on units 6-11, which correspond to the vowel in the stem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51818826-969e-41d3-b4a3-2b009ca484aa",
   "metadata": {},
   "source": [
    "***\n",
    "You're probably finding it difficult to answer Task 5 in a quantitative way from just inspecting individual patterns. How much better would it be if we could look at each category's learning progress separately over time? Voil&#224;, this is what we do in CELL7. **Run CELL7 now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a06bba-faf8-4ea2-b9ff-684be81dcd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL7\n",
    "\n",
    "train_categories=load_csv_str('PastTenseData/trainTargetCategories.csv')\n",
    "test_categories=load_csv_str('PastTenseData/testTargetCategories.csv')\n",
    "[this_network, trainerror]=network_train_only(trainInput, trainTarget, .1, 20,20, plot_error=True, cat_vector=train_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9b97c-42c7-4907-9254-028416be44d7",
   "metadata": {},
   "source": [
    "Run CELL 7 a few times to watch what happens consistently and what is specific to each simulation (remember: they are initialised with random weights).<br> \n",
    "**Task 6:** Can you explain why the different categories have different trajectories (i.e. why the error decreases faster for some categories than for others)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39661f-a047-45fd-813f-ee111397c615",
   "metadata": {},
   "source": [
    "### U-shaped curves\n",
    "\n",
    "So far, we've seen the error decreasing, mostly in a monotonous way. Where are the error curves that we said were so important for Past Tense simulations? <br>\n",
    "Firstly, remember Plunkett \\& Markman's point about macro- vs. micro-U-shapes. They said that actually the idea that there was a U-shape when averaging across all verbs was a bit misleading: that would occur if children made mistakes with all (or most) verbs simultaneously. Instead, what they found in acquisition diaries was that children would go through phases of over-regularising individual words, but not necessarily others (simultaneously). If that was the case in the neural networks, then we should only see a U-shape (decreasing error, then increasing error) for individual words, not when averaging. <br>\n",
    "So, let's plot the learning curves for individual verbs!<br>\n",
    "**Run CELL8 now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53036a-4ab3-4a0c-80ee-5320594d5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 8\n",
    "\n",
    "idx=7\n",
    "[this_network, trainerror]=network_log_pattern(trainInput, trainTarget, trainInputLabels, trainTargetLabels, .1, 20,20, idx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6db625-ae0b-4f8d-a1c1-25ecff1a1dfb",
   "metadata": {},
   "source": [
    "**Task 7** Find a few different words for each category from the excel sheet `targetLabels.csv` and substitute their index no. (remember: python starts counting at 0) in CELL8 to observe the learning progress for regular (Y,Z) vs. irregular (W,X) verbs. Can you find evidence for micro-U-shapes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2444c79-a03c-4cbe-a297-4b967329427d",
   "metadata": {},
   "source": [
    "***\n",
    "In order to evaluate the network's performance systematically, one would have to search the parameter space systematically, i.e. run lots of simulations (systematically varying learning rate, number of hidden units, number of epochs, and run each combination multiple times to be able to calculate an average trajectory for different random starting points) and observe a number of verbs from each class. This is beyond what we can do in class, but feel free to play with the network for longer in your own time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d4850-c08e-43bd-80a0-bf5422166ca3",
   "metadata": {},
   "source": [
    "***\n",
    "Well done, you've now got quite a bit of experience in modelling. Challenge yourself in the future and try to imagine how one could build a model for the experiments you encounter in your reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9672ebf8-8caf-4893-a6ca-2d2a34f3d904",
   "metadata": {},
   "source": [
    "----\n",
    "Workshop developed by Nadja Althaus\n",
    "Code adapted from https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
