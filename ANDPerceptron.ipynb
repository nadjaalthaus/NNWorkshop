{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e948db84-fd26-4936-ae81-8d9e57a3f3b7",
   "metadata": {},
   "source": [
    "# Connectionist modelling workshop \n",
    "## Perceptron for Logical AND\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d41524-19fb-481a-a37e-e0f9dc9ba3be",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this worksheet we'll build a tiny neural network to learn the *logical AND* problem.\n",
    "\n",
    "### Preparing the code and data\n",
    "\n",
    "**Run the following cell.** Please don't modify the code in this cell. This will initialise a lot of functions, and you don't have to care about these if you're a beginner, but in case someone knows python they should be able to see how this is written.\n",
    "\n",
    "For background though, the first two lines starting with `import` provide access to the existing python libraries `random`, `numpy` and `matplotlib.pyplot` which contain functions relating to randomisation, numeric calculations, and plotting, respectively. Every time we \"import a library\" like this, it's similar to including all these functions in our code.\n",
    "\n",
    "The statements beginning with `def` define custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf52bc-cd10-4afe-8496-7ab937d3eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 1\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def logical_AND_trainingdata():\n",
    "    inputs=[[0,0],[0,1],[1,0],[1,1]]\n",
    "    outputs=[0,0,0,1]\n",
    "    trainingdata={}\n",
    "    trainingdata['input']=inputs\n",
    "    trainingdata['target']=outputs\n",
    "    return trainingdata\n",
    "\n",
    "def logical_OR_trainingdata():\n",
    "    inputs=[[0,0],[0,1],[1,0],[1,1]]\n",
    "    outputs=[0,1,1,1]\n",
    "    trainingdata = {}\n",
    "    trainingdata['input'] = inputs\n",
    "    trainingdata['target'] = outputs\n",
    "    return trainingdata\n",
    "\n",
    "def logical_XOR_trainingdata():\n",
    "    inputs=[[0,0],[0,1],[1,0],[1,1]]\n",
    "    outputs=[0,1,1,0]\n",
    "    trainingdata = {}\n",
    "    trainingdata['input'] = inputs\n",
    "    trainingdata['target'] = outputs\n",
    "    return trainingdata\n",
    "\n",
    "def propagate(input, weights):\n",
    "\n",
    "    activation = weights[0] #weights[0] is bias\n",
    "\n",
    "    for i in range(len(input)):\n",
    "        activation += weights[i+1] * input[i]\n",
    "\n",
    "\n",
    "    if activation > 0.0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def get_error(target, actual_output):\n",
    "\n",
    "    error=float(target)-actual_output\n",
    "    return error\n",
    "\n",
    "def weight_update(weights,input, learningrate, error):\n",
    "    weights[0]=weights[0]+learningrate*error #bias\n",
    "    for i in range(len(weights)-1):\n",
    "        weights[i+1]=weights[i+1]+learningrate*error*input[i]\n",
    "    return weights\n",
    "\n",
    "def training_step(perceptron,index):\n",
    "    trainingdata = perceptron['trainingdata']\n",
    "    weights = perceptron['weights']\n",
    "    learningrate = perceptron['learningrate']\n",
    "\n",
    "    this_training_pattern_input = trainingdata['input'][index]\n",
    "\n",
    "    this_training_pattern_target = trainingdata['target'][index]\n",
    "\n",
    "    actual_output=propagate(this_training_pattern_input,weights)\n",
    "\n",
    "    error=get_error(this_training_pattern_target, actual_output)\n",
    "\n",
    "    weights=weight_update(weights, this_training_pattern_input, learningrate, error)\n",
    "\n",
    "    perceptron['weights']=weights\n",
    "\n",
    "\n",
    "    return perceptron, error\n",
    "\n",
    "\n",
    "\n",
    "def training_step_detailed(perceptron,index):\n",
    "    trainingdata = perceptron['trainingdata']\n",
    "    weights = perceptron['weights']\n",
    "    learningrate = perceptron['learningrate']\n",
    "\n",
    "    this_training_pattern_input = trainingdata['input'][index]\n",
    "    print('input:'+str(this_training_pattern_input))\n",
    "    this_training_pattern_target = trainingdata['target'][index]\n",
    "    print('target:'+str(this_training_pattern_target))\n",
    "    actual_output=propagate(this_training_pattern_input,weights)\n",
    "    print('actual output:'+str(actual_output))\n",
    "    error=get_error(this_training_pattern_target, actual_output)\n",
    "    print('error:'+str(error))\n",
    "    if abs(error)>0:\n",
    "        print('error, change weights!')\n",
    "    else:\n",
    "        print('correct, no need to change weights')\n",
    "    print('old weights:'+str([round(w,2) for w in weights]))\n",
    "    weights=weight_update(weights, this_training_pattern_input, learningrate, error)\n",
    "    print('new weights:'+str([round(w,2) for w in weights]))\n",
    "    perceptron['weights']=weights\n",
    "    visualise_perceptron(perceptron)\n",
    "\n",
    "    return perceptron, error\n",
    "\n",
    "\n",
    "\n",
    "def visualise_perceptron(p):\n",
    "    plt.axes()\n",
    "    tdinput=p['trainingdata']['input']\n",
    "    circle1 = plt.Circle((tdinput[0][0], tdinput[0][1]), radius=.05, fc='r')\n",
    "    circle2 = plt.Circle((tdinput[1][0], tdinput[1][1]), radius=.05, fc='r')\n",
    "    circle3 = plt.Circle((tdinput[2][0], tdinput[2][1]), radius=.05, fc='r')\n",
    "    circle4 = plt.Circle((tdinput[3][0], tdinput[3][1]), radius=.05, fc='r')\n",
    "    plt.gca().add_patch(circle1)\n",
    "    plt.gca().add_patch(circle2)\n",
    "    plt.gca().add_patch(circle3)\n",
    "    plt.gca().add_patch(circle4)\n",
    "\n",
    "    b=p['weights'][0]\n",
    "    w1=p['weights'][1]\n",
    "    w2=p['weights'][2]\n",
    "    \n",
    "    \n",
    "    a1 = -0.1\n",
    "    a2 = -0.1*(-(b/w2) / (b/w1)) + (-b / w2)\n",
    "    b1 = 1.1\n",
    "    b2= 1.1*(-(b / w2) / (b / w1)) + (-b / w2)\n",
    "    \n",
    "    line = plt.Line2D((a1,a2), (b1,b2), lw=2.5)\n",
    "    plt.gca().add_line(line)\n",
    "    plt.axis([-.1,1.1,-.1,1.1])\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "\n",
    "def display_perceptron_parameters(p):\n",
    "    tdinput=p['trainingdata']['input']\n",
    "    print('number of input units:'+str(len(tdinput[0])))\n",
    "    print('number of output units: 1')\n",
    "    print('bias:'+str(np.round(p['weights'][0],2)))\n",
    "    print('weights:'+str([np.round(w,2) for w in p['weights'][1:]]))\n",
    "    \n",
    "\n",
    "\n",
    "def initialise_perceptron(trainingdata, learningrate):\n",
    "    input=trainingdata['input']\n",
    "    target = trainingdata['target']\n",
    "    n_exemplars=len(input)\n",
    "    n_input_units=len(input[0])\n",
    "    weights=[]\n",
    "    for i in range(n_input_units+1): #+1 for bias\n",
    "        weights.append(random.random())\n",
    "    #print('starting weights:'+str([round(w,2) for w in weights[1:]]))\n",
    "    #weights=[-.5,.4, .6]# for testing\n",
    "    perceptron={}\n",
    "    perceptron['trainingdata']=trainingdata\n",
    "    perceptron['weights']=weights\n",
    "    perceptron['learningrate']= learningrate\n",
    "\n",
    "    return perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30021f75-b7b2-4063-adb1-8ca0b9a9ad05",
   "metadata": {},
   "source": [
    "The following cell will load the logical-AND training data, and it displays all the patterns in the set. **Run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531748b-8169-4830-bbc3-d157806282fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 2\n",
    "\n",
    "#load the training data\n",
    "TrainingData=logical_AND_trainingdata()\n",
    "\n",
    "#display input and target values for each pattern\n",
    "for i in range(len(TrainingData['input'])):\n",
    "    print(\"input\"+str(TrainingData['input'][i])+'-- target:'+str(TrainingData['target'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ddfd3-0188-4b86-9a73-d294207e4a2e",
   "metadata": {},
   "source": [
    "The following cell will initialise a small network, with four units (2 input - \"x\",\"y\", 1 output \"z\", and bias \"b\" as discussed in class) and weights on the 2 connections between x and z and between y and z, as well as between the bias and the output. Note it's using the functions we defined further up in order to do so. We are also setting the learning rate to 0.1 .  **Run this cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a06bba-faf8-4ea2-b9ff-684be81dcd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 3\n",
    "\n",
    "#define a variable for the learning rate\n",
    "lr=.1\n",
    "\n",
    "#initialise a perceptron called \"ANDperceptron\"\n",
    "ANDperceptron=initialise_perceptron(TrainingData, lr)\n",
    "\n",
    "#display the values in the perceptron\n",
    "display_perceptron_parameters(ANDperceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9b97c-42c7-4907-9254-028416be44d7",
   "metadata": {},
   "source": [
    "This previous cell contained calls to funtions that use randomisation. This is because we want every instance of a network to be slightly different. Some initial settings are going to be closer to a good solution, and some are going to be worse. \n",
    "Ignore the bit that says \"bias\" for now.\n",
    "**Run the same cell again and watch the values change.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39661f-a047-45fd-813f-ee111397c615",
   "metadata": {},
   "source": [
    "### Training a network\n",
    "\n",
    "Now let's train the network. Because we know (from Rosenblatt's work) that perceptrons always converge for linearly separable problems like AND (i.e. the error will go down to 0), we set this up to run until error=0 over the whole training set is reached, i.e. the network gets all patterns correct. Presenting the whole training set once is called an \"epoch\", so we are watching for the error for an epoch to reach 0.\n",
    "The following cell runs one entire training run for this network. **Run the cell now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc53036a-4ab3-4a0c-80ee-5320594d5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 4\n",
    "\n",
    "epoch_e=np.nan  #set up a variable to log the combined epoch error\n",
    "epoch_count=0 #set up a variable to count epochs\n",
    "while epoch_e!=0.0:  #a while-loop: keep iterating the following, until the epoch error is 0\n",
    "    epoch_count=epoch_count+1  #increase the epoch counter\n",
    "    e=[]   #initialise an empty list to hold the error for each training pattern in this epoch\n",
    "    seq=[0,1,2,3]\n",
    "    shuffleseq=np.random.permutation(seq)  #randomise the presentation order for this epoch\n",
    "\n",
    "    for j in range(4):  #a for-loop: do the following 4 times, i.e. for each training pattern\n",
    "        [ANDperceptron,e0]=training_step(ANDperceptron,shuffleseq[j]) #run one training step\n",
    "        e.append(e0) #append the error for this pattern at the end of the list of errors\n",
    "    epoch_e=np.sum([abs(err) for err in e]) #sum up errors for all patterns in this epoch\n",
    "\n",
    "    print('epoch'+str(epoch_count)+' error:'+str(epoch_e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6db625-ae0b-4f8d-a1c1-25ecff1a1dfb",
   "metadata": {},
   "source": [
    "Train the network a few times. To do this:<br>\n",
    "**Re-run the cell that initialises the network (No. 3)**<br>\n",
    "then<br>\n",
    "**Re-run the cell that trains the network (No. 4)**<br>\n",
    "***\n",
    "**Task 1**: Does the error decrease the same way every time? Is it sometimes faster/less fast?<br>\n",
    "\n",
    "Why is that the case? Let's break it down.  The following cell initialises a network, and the one below runs one training step, but with a bit more information about each step. This time it's your turn to run each training step - run the cell multiple times, until you get error = 0. Each time you should change the value of the variable i, to either 0,1,2 or 3, to select one of the training values.\n",
    "\n",
    "Setting\n",
    "`i = 0` will pick the (0,0) training pattern<br>\n",
    "`i = 1` will pick the (0,1) training pattern<br>\n",
    "`i = 2` will pick the (1,0) training pattern<br>\n",
    "`i = 3` will pick the (1,1) training pattern<br>\n",
    "\n",
    "Note that it is your job to make sure every pattern gets presented equally often - you may want to keep track on a piece of paper!<br>\n",
    "**Task 2:** Use the next two cells (No. 5 and 6) to log training in detail. What does the plot show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05e0a9-f12c-4993-baab-75f7aeeba0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 5\n",
    "\n",
    "#define a variable for the learning rate\n",
    "lr=.1\n",
    "\n",
    "#initialise a perceptron called \"ANDperceptron\"\n",
    "ANDperceptron=initialise_perceptron(TrainingData, lr)\n",
    "\n",
    "#display the values in the perceptron\n",
    "display_perceptron_parameters(ANDperceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf4a52-287e-4cad-b47f-1ed15e73e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 6\n",
    "\n",
    "i = 3 #INSERT TO THE LEFT a number between 0 and 3 to select the training pattern\n",
    "[ANDperceptron,e]=training_step_detailed(ANDperceptron,i)\n",
    "print('error:'+str(abs(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4e4e8-6044-4217-8170-a1124c0e6004",
   "metadata": {},
   "source": [
    "Observe how the weights change over training, and that they are only adjusted when there is an error for a pattern.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ecbd8-aa85-45fb-b920-c3db2fd11547",
   "metadata": {},
   "source": [
    "### The role of the learning rate\n",
    "\n",
    "Now, let's consider the role of the learning rate. This represents the amount by which weights are changed on every round. In this task, you should try to systematically observe how changing the learning rate affects how fast/slowly the network can learn.\n",
    "The following cell initialises a network and then runs training (just like above). Run it multiple times, and adjust the learning parameter where indicated. You should try a parameter range starting at .001, via .01, to .1 (what we've been using so far), .2, .3 (or more values if you like.\n",
    "Keep track of the number of epochs it takes your network to reach epoch error=0 (e.g. on a piece of paper). What's the best value? Is this robust (i.e. if you run it again with the same learning rate, do you get the same result? <br>\n",
    "**Task 3:** Run this cell multiple times and adjust the code to change the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422d3200-c883-436a-8067-97a69a62d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 7\n",
    "\n",
    "#define a variable for the learning rate\n",
    "lr=   .2 #INSERT TO THE LEFT a value for the learning rate\n",
    "\n",
    "#initialise a perceptron called \"ANDperceptron\"\n",
    "ANDperceptron=initialise_perceptron(TrainingData, lr)\n",
    "\n",
    "#display the values in the perceptron\n",
    "display_perceptron_parameters(ANDperceptron)\n",
    "\n",
    "\n",
    "epoch_e=np.nan  #set up a variable to log the combined epoch error\n",
    "epoch_count=0 #set up a variable to count epochs\n",
    "while epoch_e!=0.0:  #a while-loop: keep iterating the following, until the epoch error is 0\n",
    "    epoch_count=epoch_count+1  #increase the epoch counter\n",
    "    e=[]   #initialise an empty list to hold the error for each training pattern in this epoch\n",
    "    seq=[0,1,2,3]\n",
    "    shuffleseq=np.random.permutation(seq)  #randomise the presentation order for this epoch\n",
    "\n",
    "    for j in range(4):  #a for-loop: do the following 4 times, i.e. for each training pattern\n",
    "        [ANDperceptron,e0]=training_step(ANDperceptron,shuffleseq[j]) #run one training step\n",
    "        e.append(e0) #append the error for this pattern at the end of the list of errors\n",
    "    epoch_e=np.sum([abs(err) for err in e]) #sum up errors for all patterns in this epoch\n",
    "\n",
    "    print('epoch'+str(epoch_count)+' error:'+str(epoch_e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23011052-65d5-4f8e-b2f3-1241351b05a6",
   "metadata": {},
   "source": [
    "### Learning different logical functions\n",
    "\n",
    "Logical AND isn't the only tiny network we could run --- we can use the exact same network for the logical OR network (where the output unit is meant to \"fire\" as soon as at least 1 input is \"on\", i.e. output 1 for (0,1), (1,0) and (1,1) and output 0 only for (0,0).\n",
    "\n",
    "The following cell loads the OR training set, initialises a network, and trains it. <br>\n",
    "**Task 4:** Run this a few times. How does training compare to the AND network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d8999-dc8c-4270-92d1-8ca4ac23d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL 8\n",
    "\n",
    "TrainingData=logical_OR_trainingdata()\n",
    "\n",
    "#display input and target values for each pattern\n",
    "for i in range(len(TrainingData['input'])):\n",
    "    print(\"input\"+str(TrainingData['input'][i])+'-- target:'+str(TrainingData['target'][i]))\n",
    "\n",
    "#define a variable for the learning rate\n",
    "lr=.1   \n",
    "\n",
    "#initialise a perceptron called \"ANDperceptron\"\n",
    "ANDperceptron=initialise_perceptron(TrainingData, lr)\n",
    "\n",
    "#display the values in the perceptron\n",
    "display_perceptron_parameters(ANDperceptron)\n",
    "\n",
    "print('----start training----')\n",
    "\n",
    "epoch_e=np.nan  #set up a variable to log the combined epoch error\n",
    "epoch_count=0 #set up a variable to count epochs\n",
    "while epoch_e!=0.0:  #a while-loop: keep iterating the following, until the epoch error is 0\n",
    "    epoch_count=epoch_count+1  #increase the epoch counter\n",
    "    e=[]   #initialise an empty list to hold the error for each training pattern in this epoch\n",
    "    seq=[0,1,2,3]\n",
    "    shuffleseq=np.random.permutation(seq)  #randomise the presentation order for this epoch\n",
    "\n",
    "    for j in range(4):  #a for-loop: do the following 4 times, i.e. for each training pattern\n",
    "        [ANDperceptron,e0]=training_step(ANDperceptron,shuffleseq[j]) #run one training step\n",
    "        e.append(e0) #append the error for this pattern at the end of the list of errors\n",
    "    epoch_e=np.sum([abs(err) for err in e]) #sum up errors for all patterns in this epoch\n",
    "\n",
    "    print('epoch'+str(epoch_count)+' error:'+str(epoch_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804767e5-4425-4d92-be0f-918dad8b03ff",
   "metadata": {},
   "source": [
    "***\n",
    "Finally, let's try this with XOR (exclusive or), where the output unit is meant to \"fire\" if exactly one input is \"on\", i.e. output 1 for (0,1) and (1,0) but output 0 for (1,1) and (0,0).\n",
    "\n",
    "WARNING: Wait for the rest of the class to press the button on this one. It could disrupt your computer for a very long time. If you need to stop the process, click on \"Kernel\" --> \"Interrupt\" in the menu.\n",
    "\n",
    "Why does it not stop training?\n",
    "The answer is, we told python to keep repeating the procedure until the error is 0. This is never reached. As Minsky and Papert showed in their famous book in 1969, perceptrons can only handle a small class of problems, the so-called linearly-separable problems. XOR is not linearly separable, so the network can never completely reduce the error.\n",
    "\n",
    "Next week, we'll therefore work with the more powerful neural networks that contain a hidden layer!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be261d-d1f1-4986-89e2-2affda685bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData=logical_XOR_trainingdata()\n",
    "\n",
    "#display input and target values for each pattern\n",
    "for i in range(len(TrainingData['input'])):\n",
    "    print(\"input\"+str(TrainingData['input'][i])+'-- target:'+str(TrainingData['target'][i]))\n",
    "\n",
    "#define a variable for the learning rate\n",
    "lr=.1   \n",
    "\n",
    "#initialise a perceptron called \"ANDperceptron\"\n",
    "ANDperceptron=initialise_perceptron(TrainingData, lr)\n",
    "\n",
    "#display the values in the perceptron\n",
    "display_perceptron_parameters(ANDperceptron)\n",
    "\n",
    "print('----start training----')\n",
    "\n",
    "epoch_e=np.nan  #set up a variable to log the combined epoch error\n",
    "epoch_count=0 #set up a variable to count epochs\n",
    "while epoch_e!=0.0:  #a while-loop: keep iterating the following, until the epoch error is 0\n",
    "    epoch_count=epoch_count+1  #increase the epoch counter\n",
    "    e=[]   #initialise an empty list to hold the error for each training pattern in this epoch\n",
    "    seq=[0,1,2,3]\n",
    "    shuffleseq=np.random.permutation(seq)  #randomise the presentation order for this epoch\n",
    "\n",
    "    for j in range(4):  #a for-loop: do the following 4 times, i.e. for each training pattern\n",
    "        [ANDperceptron,e0]=training_step(ANDperceptron,shuffleseq[j]) #run one training step\n",
    "        e.append(e0) #append the error for this pattern at the end of the list of errors\n",
    "    epoch_e=np.sum([abs(err) for err in e]) #sum up errors for all patterns in this epoch\n",
    "\n",
    "    print('epoch'+str(epoch_count)+' error:'+str(epoch_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46853d16-f850-4d57-8aac-1b87b5cb02dd",
   "metadata": {},
   "source": [
    "--------\n",
    "Worksheet developed by Nadja Althaus.\n",
    "Code adapted from https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
